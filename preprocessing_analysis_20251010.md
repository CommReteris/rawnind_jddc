Detailed Preprocessing Operations Analysis
Overview of the Two Processing Tracks
The paper presents two parallel approaches that differ fundamentally in where demosaicing occurs within the pipeline. The raw Bayer track operates on native sensor data arranged in a four-channel format, while the linear RGB track first demosaics to three-channel images before further processing. Both tracks ultimately produce outputs that can integrate into standard raw development workflows, but they make different computational and generalization tradeoffs.
Raw Bayer Track Preprocessing
The raw Bayer preprocessing pipeline begins with fundamental sensor data cleanup operations. The pipeline first removes empty borders from the raw sensor data, which many cameras include as padding around the active pixel area. The preprocessing then subtracts the black level, which represents the sensor's baseline output in complete darkness, and normalizes all pixel values to the sensor's white level, effectively scaling the data to a zero-to-one range that represents the sensor's full dynamic range.
A critical standardization step involves cropping the Bayer pattern to achieve consistency across different camera sensors. Since different manufacturers arrange color filter arrays in various patterns including RGGB, BGGR, GRBG, and GBRG, the preprocessing crops up to two rows or columns from the image edges to ensure all data conforms to the RGGB pattern. This operation, illustrated in Figure 4 of the paper, enables the model to work with a consistent channel ordering regardless of the source camera.
The standardized Bayer data is then divided into overlapping patches of 512×512 pixels with a stride of 128 pixels. This overlap ensures that the model encounters various spatial contexts during training and helps prevent artifacts at tile boundaries during inference. The relatively small patch size accommodates the limited memory available on typical training hardware while still providing sufficient spatial context.
Linear RGB Track Preprocessing
The linear RGB preprocessing begins with demosaicing, converting the single-color-per-pixel Bayer mosaic into full three-channel RGB images. The paper employs an edge-aware interpolation method referenced as Li's algorithm for standard Bayer patterns, and switches to the Markesteijn algorithm specifically for Fujifilm's X-Trans sensor pattern, which has a more complex non-repeating arrangement that requires specialized demosaicing.
Following demosaicing, the preprocessing performs a color space transformation from the camera-native CamRGB space to the standardized linear Rec. 2020 color space. This transformation relies on camera-specific matrices extracted from EXIF metadata that describe the relationship between the sensor's color response and the CIE XYZ color space under D65 daylight illumination. The transformation chains these matrices as shown in Equation 1 of the paper, first converting from CamRGB to XYZ space, then from XYZ to Rec. 2020.
The linear RGB images are divided into larger overlapping patches of 1024×1024 pixels with a stride of 256 pixels. The larger patch size reflects that RGB data contains four times as many total values as the corresponding Bayer representation at half the spatial resolution in each dimension.
Image Alignment Procedure
Both processing tracks require precise alignment between noisy and clean image pairs, but alignment must occur in the RGB domain rather than the Bayer domain because individual Bayer pixels contain only one color channel and cannot be meaningfully compared across translations. The alignment algorithm employs an iterative local search that explores a 3×3 pixel neighborhood around the current best alignment, searching for the translation that minimizes the L1 loss between gain-normalized RGB images.
The algorithm begins from an initial alignment estimate and continues searching within local neighborhoods until no improvement can be found within the 3×3 window or the maximum allowable shift of ±128 pixels in either direction is reached. Before computing the L1 loss, the algorithm normalizes the gain between image pairs to account for brightness differences between the noisy and clean captures. The resulting optimal shift and its corresponding loss value are recorded with each image pair, and pairs exhibiting alignment loss exceeding 0.035 are discarded from the dataset, which eliminates approximately 2.4 percent of captured pairs.
Loss Masking Strategy
The preprocessing implements a sophisticated masking strategy to exclude problematic regions from loss computation during training. Binary masks identify pixels where the alignment loss exceeds 0.4 or surpasses the 99.99th percentile of the loss distribution across the image. These regions typically correspond to moving objects, lighting changes between captures, or transient elements like insects that appeared in only one image of the pair.
The masking also excludes overexposed pixels where the ground truth image value reaches or exceeds 0.99, representing regions that have saturated the sensor and contain no useful gradient information for training. After identifying problematic pixels, the preprocessing applies a binary opening operation, which is a morphological operation that first erodes and then dilates the mask to remove small isolated regions and create smoother, more coherent masked areas. Finally, any image crops where more than 50 percent of the area falls under the mask are excluded entirely from training.
Training Data Loading Adaptations
When loading aligned image pairs during training, the preprocessing must handle the different spatial resolutions between Bayer and RGB representations. For RGB-Bayer pairs, the Bayer shift value is halved using integer division since the Bayer representation operates at half the spatial resolution. When the shift value is odd, the preprocessing trims an additional row or column from both the RGB and Bayer images to maintain proper alignment after the integer division.
Before extracting training crops, the preprocessing matches the average pixel intensities between clean and noisy images by computing a gain normalization factor. This accounts for slight exposure differences between the paired captures. The loader then extracts random 256×256 pixel crops from the RGB images, or equivalently sized 128×128 crops from the corresponding Bayer data.
Color Space Transformation Timing
A critical architectural detail distinguishes how color space transformation integrates into the two tracks. For Bayer models, the network processes four-channel Bayer data throughout all internal layers and only outputs three-channel CamRGB data after a final PixelShuffle upsampling layer. The CamRGB-to-Rec2020 transformation is applied to this output during training before computing the loss function, which ensures the model learns to produce outputs that align well with the target color space while allowing the network to operate in the native sensor color space.
Linear RGB models receive pre-transformed Rec. 2020 data as input and produce Rec. 2020 output directly, with no color space transformation in the training loop. This difference reflects the philosophical distinction between the two approaches: Bayer models preserve sensor-native characteristics throughout processing while standardizing only at the output stage, whereas linear RGB models standardize the input representation to facilitate cross-sensor generalization.
Developed Image Track (Comparison Baseline)
To enable fair comparison with methods that operate on developed images, the preprocessing includes a custom proxy development pipeline that simulates typical image processing operations. This pipeline applies logarithmic tone mapping to the luminance channel to compress the high dynamic range of linear data into a displayable range. Edge enhancement uses a Laplacian filter to increase local contrast, followed by gamma correction that applies a nonlinear power function to match display characteristics.
Contrast enhancement employs a sigmoid function to stretch or compress value ranges, and image sharpening combines Gaussian blur with weighted blending to accentuate fine details. Importantly, all parameters for these operations are randomized within plausible ranges during training to expose the model to diverse development styles. While this proxy pipeline does not replicate the full complexity of commercial raw processors like Adobe Lightroom or Darktable, it provides consistent training data that enables controlled comparison across the different input representations.
Clean Data Augmentation Strategy
To improve generalization across diverse camera sensors, the training dataset incorporates unpaired clean images captured at ISO 200 or below. The augmentation includes 561 Bayer images sourced from the raw.pixls.us community repository spanning numerous camera models, plus 11,815 Bayer images from the researchers' personal collection captured with several cameras. These clean images expose the model to a broader range of sensor characteristics without requiring paired noisy captures for every sensor variant, which would be prohibitively expensive to collect.
Preprocessing Variants and Their Outcomes
The ablation study explores several preprocessing variations to understand their impact on model performance. The extra pairs variant incorporates additional noisy-clean image pairs specifically from the Canon EOS M100, which serves as the unknown sensor in testing. This variant achieves an MS-SSIM of 0.876 on the unknown sensor test set, compared to 0.873 when training with clean data augmentation and 0.871 without clean data, demonstrating that targeted data collection provides marginal benefits over diversity-based generalization.
The no clean data variant excludes all unpaired clean images during training, relying exclusively on the noisy-clean pairs from the primary dataset. For Bayer models, this reduces performance slightly, suggesting that exposure to diverse sensors through clean data aids generalization. Interestingly, linear RGB models show no performance difference with or without clean data augmentation, indicating that the standardized color space already provides sufficient generalization capability.
The pre-upsampled variant applies bilinear interpolation to upsample Bayer data to full RGB resolution before feeding it to the model. This increases computational complexity by operating on four times as many pixels but simplifies the model architecture by removing the need for PixelShuffle upsampling at the output. Performance falls between native Bayer processing and linear RGB processing, suggesting that simple interpolation introduces some artifacts that the model must work around, though fewer than the complex nonlinearities of full image development.
The gamma correction variant applies a power function with exponent 1/2.2 to model outputs before computing the loss, hypothetically aligning the training objective with perceptual color space. However, this variant performs poorly, with Bayer models experiencing a 0.6 percent MS-SSIM drop and training instability issues. The failure likely stems from the mismatch between gamma-corrected outputs and the intrinsically linear characteristics of raw sensor data that the model learns to preserve.
The more channels variant expands the model's channel count by 1.5× to test whether capacity limitations constrain performance. The results show minimal improvement, suggesting that the base architecture already provides sufficient capacity for the denoising task and that performance limitations arise from other factors such as dataset diversity or fundamental algorithmic constraints rather than model size.
Evaluation Methodology Considerations
The evaluation methodology introduces an important asymmetry that must be understood when interpreting results. Raw model outputs undergo development using manually curated Darktable settings before MS-SSIM computation, while models trained on developed images have their outputs evaluated directly. This approach reflects real-world usage patterns where raw processors always apply some development pipeline, but it means that raw model outputs must survive an additional nonlinear transformation that could amplify certain types of artifacts differently than the development applied to their inputs.
The paper acknowledges this potential advantage for developed-image models but argues that applying consistent development to all outputs represents the fairest possible comparison given that raw data lacks immediate visual interpretability. The manually curated XMP sidecar files containing development settings are published alongside the dataset to enable reproducibility and community validation of this evaluation approach.
Critical Pitfalls for Replication
Several aspects of this preprocessing pipeline contain subtle complexities that could derail replication attempts if not handled with extreme care.
Alignment Domain Constraint
The alignment must occur in the RGB domain after demosaicing, never in the Bayer domain. This constraint arises because individual Bayer pixels contain only one color channel, making it impossible to compute meaningful intensity differences across spatial translations. Attempting alignment in the Bayer domain would essentially compare red pixels to green or blue pixels at different positions, producing nonsensical results. The preprocessing must first demosaic both images using identical algorithms before computing alignment shifts, even for models that will ultimately train on Bayer data.
Bayer Pattern Standardization Complexity
Converting arbitrary Bayer patterns to RGGB requires careful attention to the starting position within the color filter array. The crop must remove the minimum number of rows and columns necessary to achieve RGGB ordering, which means understanding the exact pattern of the source sensor and determining whether zero, one, or two rows or columns must be removed from each edge. Getting this wrong would cause color channels to be misaligned throughout the training pipeline, likely causing the model to produce images with severe color artifacts.
Integer Division and Odd Shift Handling
When converting RGB alignment shifts to Bayer coordinates, the integer division can create misalignment if the original shift was odd. The preprocessing handles this by detecting odd shifts and trimming an additional row or column, but this logic must be implemented correctly for all four possible combinations of odd/even horizontal and vertical shifts. Missing even one case would cause intermittent misalignment in the training data that might be difficult to diagnose since most image pairs would still align correctly.
Color Transformation Matrix Extraction
The color space transformation relies on extracting specific matrices from EXIF metadata that describe the camera's color response under D65 illumination. Different raw file formats store this information in different EXIF tags, and some cameras may not provide the necessary matrices at all. The preprocessing must validate that it has correctly extracted and inverted the XYZ-to-CamRGB matrix before computing the CamRGB-to-Rec2020 transformation, and must handle cases where this data is missing or invalid.
Bayer Model Output Transformation Timing
For Bayer models, the color space transformation must occur after PixelShuffle upsampling but before loss computation during training. Applying the transformation at any other point in the network would cause the model to learn incorrect color relationships. During inference, the transformation must be omitted since the output should remain in CamRGB space for integration into standard raw development workflows. This asymmetry between training and inference could easily be overlooked.
Loss Masking Implementation Subtleties
The loss masking involves multiple sequential operations that must be applied in the correct order. Computing the 99.99th percentile threshold, applying the overexposure threshold, performing binary opening, and checking the 50 percent masked area criterion all require careful implementation. The binary opening operation in particular requires choosing appropriate structuring element size and ensuring the implementation matches standard morphological operations from image processing libraries.
Training Instability for Certain Configurations
The paper explicitly notes that training Bayer compression-only models proved unreliable, with models failing to converge or producing unstable results. This suggests that certain architectural configurations require the regularization effect of joint denoising to achieve stable optimization. Replication attempts that separate denoising and compression for Bayer data should anticipate potential training difficulties and may need to adjust learning rates, regularization parameters, or optimization algorithms.
Evaluation Pipeline Asymmetry
The evaluation methodology applies an additional development pipeline to raw model outputs but evaluates developed-image model outputs directly. Replication attempts must decide whether to adopt this same asymmetric evaluation or to use a different approach, recognizing that either choice introduces certain advantages or disadvantages for particular model types. The choice significantly affects the interpretation of performance comparisons.
Gamma Correction Counterintuitive Failure
The paper demonstrates that applying gamma correction before loss computation actually harms performance and training stability, which contradicts common intuitions about perceptual color spaces. Replication attempts should avoid this approach despite its apparent theoretical motivation, and should be aware that other perceptually-motivated transformations might similarly interfere with learning effective representations of linear raw data characteristics.
Patch Size and Stride Interdependencies
The different patch sizes and strides for Bayer versus RGB data must maintain exact correspondence in their overlapping regions. The 512×512 Bayer patches with 128-pixel stride correspond exactly to 1024×1024 RGB patches with 256-pixel stride due to the 2× spatial resolution difference. Using different ratios would cause misalignment in the extracted training data that could degrade performance in subtle ways.
Clean Data Augmentation Source Reliability
The clean data augmentation draws from community repositories and personal collections where capture parameters and calibration quality may vary. The ISO 200 threshold attempts to ensure high quality, but replication attempts should validate that augmentation images actually meet quality standards and should be prepared to filter additional problematic images based on noise levels, focus quality, or other criteria.
Development Pipeline Randomization Range
The proxy development pipeline randomizes parameters within "plausible ranges" that the paper does not specify precisely. Replication attempts must make subjective judgments about what constitutes plausible variation for tone mapping curves, sharpening strength, contrast enhancement, and other parameters. Choosing ranges that are too narrow may not provide sufficient training diversity, while excessively wide ranges could expose the model to unrealistic development styles that hurt generalization to typical use cases.
